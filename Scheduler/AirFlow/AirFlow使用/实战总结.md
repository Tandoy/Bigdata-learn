#注意事项

1.airflow采用python解析HQL

  1.1 传参问题：可将复杂sql拆分多行 " \进行分割，然后{0}.format()形式进行传参
              也可采用'%s' %etl_date进行传参

  1.2 python解析问题：

      1.2.1 首先sql中不能存在注释

      1.2.2 存在严格缩进写法不然无法解析

      1.2.3 注意sql中所有双引号改为单引号不然解析报错

      1.2.4 建议行尾" \分割符书写时每行自动空格避免解析报错
      
      1.2.5 需要回溯历史数据时，跑批日期不应设置为：datetime.now()，它默认是读取服务器同步时间
            而不是实际的回溯跑批日期，而应设置为：
          T:
            etl_date="{{macros.ds_format(ds,'%Y-%m-%d','%Y%m%d')}}"
            etl_mth="{{macros.ds_format(ds,'%Y-%m-%d','%Y')}}"+"{{macros.ds_format(ds,'%Y-%m-%d','%m')}}"
          T-2
            etl_date="{{macros.ds_format(macros.ds_add(ds,-2),'%Y-%m-%d','%Y%m%d')}}"
            etl_mth="{{macros.ds_format(macros.ds_add(ds,-2),'%Y-%m-%d','%Y')}}"+"{{macros.ds_format(macros.ds_add(ds,-2),'%Y-%m-%d','%m')}}"
  
2.airflow指定hive参数问题

  2.1 由于业务场景需要常常需要进行复杂HQL的书写，但由于测试环境与生产环境的差异往往跑批过程中出现OOM、数据倾斜等问题
  
   这是便需要设定hive参数进行调优处理。在hue中可直接书写，但airflow需在dag中进行指定
      
      sqlccdbiz = SQL_stg_card_ccdbiz_cu()
      stg_card_ccdbiz_cu = HiveOperator(
            task_id = "stg_card_ccdbiz_cu",
            hql = sqlccdbiz.sqlstg_ccdbiz,
            hiveconfs={'mapreduce.map.java.opts':'-Xmx3072m','hive.auto.convert.join':'false'},
            hive_cli_conn_id = 'hive',
            dag = dag
      	)
3.airflow参数

  3.1 retries：是指任务失败后重试次数
  
  3.2 retry_delay：重试间隔
  
  若存在复杂业务HQL，自行预估会影响集群yarn分配资源的大小以及时长，可设置重试次数以及重试间隔
  
4.airflow调优
  #####并行度参数
  4.1 parallelism ：这是用来控制每个airflow worker 可以同时运行多少个task实例。这是airflow集群的全局变量。在airflow.cfg里面配置
  
  4.2 concurrency ：这个用来控制 每个dag运行过程中最大可同时运行的task实例数。如果你没有设置这个值的话，scheduler 会从airflow.cfg里面读取默认值 dag_concurrency

  4.3 max_active_runs : 这个是用来控制在同一时间可以运行的最多的dag runs 数量
  
  示例代码：
        
        import datetime as dt
        import logging
        
        from airflow.operators.dummy_operator import DummyOperator
        
        import config
        
        from airflow import DAG
        from airflow.operators.python_operator import PythonOperator
        
        default_args = {
            'owner': 'airflow',
            'depends_on_past': True,
            'wait_for_downstream': True,
        }
        
        
        dag = DAG(
            dag_id='test_parallelism_dag',
            default_args=default_args,
            max_active_runs=1,
            concurrency=30
        )
        
        def print_operators(ds, **kwargs):
            logging.info(f"Task {kwargs.get('task_instance_key_str', 'unknown_task_instance')}")
        
        
        print_operators = [PythonOperator(
            task_id=f'test_parallelism_dag.print_operator_{i}',
            python_callable=print_operators,
            provide_context=True,
            dag=dag
        ) for i in range(60)]
        
        dummy_operator_start = DummyOperator(
            task_id=f'test_parallelism_dag.dummy_operator_start',
        )
        
        dummy_operator_end = DummyOperator(
            task_id=f'test_parallelism_dag.dummy_operator_end',
        )
        
        dummy_operator_start >> print_operators >> dummy_operator_end

5.airflow部分dag失败后进行重跑步骤
  
  例如 dagA 中， T1 >> T2 >> T3 >> T4 >> T5  ，其中 T1 T2 成功， T3 失败， T4  T5因为依赖 T3，也不会运行。
  
  跳过 T1   T2  重跑 T3 T4 T5 的方法是：
  
  （1）点击 T3 clear ，默认会clear T4  T5 的状态
  
  （2）返回到上一级，点击 dagA 名字，点击 Recursive 和 Downstream （airflow的界面，看不出来到底有没有选择，反正各点一次，  Uncheck ），然后点击 run 
  
6.airflow重跑某一天步骤
  
  6.1 airflow UI界面点击Tree View，然后找到具体某一天点击Clear即可
  
7.airflow读取配置文件

  7.1 在airflow安装目录../airflow/home/ 下会有airflow.cfg文件，我们可将RMDB相关连接信息放在此处
  
    [airflow_ex]
    host = XXXX
    username = XXXX
    password = XXXX
    database = XXXX
    
  7.2 读取airflow.cfg文件中配置文件内容脚本如下：
  
    from airflow_cfg import GET_CONN
    mysql_db = kwargs['mysql_db']
    host = get_conn(mysql_db,'host')
    username = get_conn(mysql_db,'username')
    password = get_conn(mysql_db,'password')
    database = get_conn(mysql_db,'database')
    conn = pymysql.connect(host,username,password,database)
    curs = conn.cursor()
    ......
    