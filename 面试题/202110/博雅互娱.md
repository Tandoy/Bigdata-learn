一、深圳博雅互娱 

20211011 下午四点
```text
一面：
1.自我介绍
2.scala会多少，python是否区分大小写
    区分
3.CDH是否亲自搭建过
4.Spark dataframe与dataset的区别
    DataFrame是一种分布式数据集合，数据以命名列的形式组织，类似于关系型数据库表。而Dataset是DataFrame API的扩展，提供了更多的功能，如类型安全和面向对象的接口。
5.Spark RDD是弹性分布式数据集，如何理解弹性？
    指的是其具有容错性和恢复能力
6.如果线上出现一个hive任务跑批很慢，你有什么解决方式？
    首先确定是不是集群现在是不是资源紧张，如果只是单个任务的话考虑数据倾斜处理
7.用java写过一些spark代码嘛？
8.使用Spark离线跑批与Hive有什么不同？性能？语法？
    Hive是使用的MR引擎，shuffle中途会有落盘计算操作，spark一般来说是基于内存计算会更快。并且在sql语法上会有差异，并不是兼容
9.说说这个实时数仓项目，它解决了那些问题？
10.为什么选用Flink？
    不是准实时是需要数据流驱动的实时
11.Spark Structured Streaming有使用过吗？
12.MR shuffle 与 Spark shuffle的差异有哪些？
   MR（MapReduce）和Spark的shuffle操作都是在数据处理过程中进行数据重分布的过程。在MR中，shuffle操作是指将Mapper任务产生的数据传输给Reducer任务的过程。而在Spark中，shuffle操作是指在两个阶段之间重新分布数据以便将具有相同键值对的数据分组到一个分区中的过程。
    在MR中，shuffle操作由Mapper任务产生数据的源执行器和Reducer任务消费数据的目标执行器之间的数据传输完成。而在Spark中，shuffle操作是在宽依赖或重新分区时发生的。
    在MR中，shuffle操作由ShuffleManager控制，而在Spark中，ShuffleManager是SparkEnv在驱动程序和每个执行器上创建的，用于注册shuffle并允许执行器读取和写入数据。
    在MR中，shuffle操作的实现取决于Hadoop版本，而在Spark中，ShuffleManager是一个可插拔的接口，可以根据需要实现自定义的shuffle管理器接口。
    在MR中，shuffle操作的结果是创建输出文件，而在Spark中，ShuffleMapStage的结果是创建输出文件，用于提供数据给后续阶段。
    在MR中，shuffle操作的任务称为ShuffleMapTask，而在Spark中，ShuffleMapTask执行三个操作：读取数据、计算输出分区和写入shuffle文件。
    在MR中，shuffle操作使用Partitioner来决定输出分区，而在Spark中，有两种Partitioner的实现：HashPartitioner和RangePartitioner。
    在MR中，shuffle操作的写入由ShuffleWriter完成，而在Spark中，有三种ShuffleWriter的实现：BypassMergeSortShuffleWriter、SortShuffleWriter和UnsafeShuffleWriter。
   总的来说，MR和Spark的shuffle操作都是数据重分布的过程，但在实现细节和执行方式上有一些差异。
13.是否看过相关开源项目的源码？可以说一下其亮点设计吗？
14.Hadoop 3.X 与 Hadoop 2.X的区别，新增了那些新特性？生产上使用的Spark版本？
15.有什么想问我的？

20211013上午十一点半 VP面
VP面：
1.项目介绍
2.所使用的技术栈
3.个人发展方向
4.日常工作内容
5.期望薪资
6.有什么想问我的？
```